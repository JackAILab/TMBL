# Multimodal Sentiment Analysis
This repository records the proposed Multimodal Sentiment Analysis on Pytorch Framework
## Trackers
Here is a list of the improved trackers.

* MISA: Misa: Modality-invariant and-specific representations for multimodal sentiment analysis, ACMM, 2020. [[Paper]](https://dl.acm.org/doi/abs/10.1145/3394171.3413678)
* MMIM: Progressive image deraining networks: A better and simpler baseline, arXiv, 2021. [[Paper]](https://arxiv.org/abs/2109.00412)
* HyCon: Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis, IEEE Transactions on Affective Computing, 2022. [[Paper]](https://ieeexplore.ieee.org/abstract/document/9767560)  
* TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis, Pattern Recognition, 2023. [[Paper]](https://www.sciencedirect.com/science/article/pii/S0031320322007385)
* AOBERT: All-modalities-inOne BERT for multimodal sentiment analysis, 2023. [[Paper]](https://www.sciencedirect.com/science/article/pii/S1566253522002329)  

## Findings
* In this study, we develop a transformer model equipped with a modality-bound learning mechanism to extract unified affective information from multiple modalities. Our proposed structure and method hold potential for broader applications in other fields related to multimodal interaction and fusion, such as multimodal disease diagnosis.









  
